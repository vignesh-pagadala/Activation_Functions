{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Activation Functions: An Examination of the Rectified Linear Function and its Performance </center>\n",
    "<center> Vignesh M. Pagadala </center>\n",
    "<center> Department of Computer Science </center>\n",
    "<center> Colorado State University </center>\n",
    "<center> Vignesh.Pagadala@colostate.edu </center>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Contents </center>\n",
    "***\n",
    "1. Abstract\n",
    "2. Rectified Linear Unit\n",
    "> - Description\n",
    "> - Implementation\n",
    "3. Performance in Comparision with tanH\n",
    "> - Plot\n",
    "> - Observations\n",
    "> - Inference\n",
    "4. References\n",
    "5. Extra Credit Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Abstract\n",
    "<p>\n",
    "    Presently, one of the most popular activation functions in use for training neural networks is the Rectified Linear Unit, abbreviated as ReLU. In this report, we perform an experiment to examine the performance of a neural network with ReLU used as the activation function, and compare it with the performance when the hyperbolic tangent function is used in the same capacity. We initially define functions implementing ReLU, and apply it to train several different neural network configurations, and also do the same using the hyperbolic tangent function. We finally calculate the Root Mean Squared Error (RMSE), plot the results, observe and infer.  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import neuralnetworksA2 as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rectified Linear Unit <br>\n",
    "\n",
    " - ### Description <br> \n",
    "    The Rectified Linear function is described as follows. If the input is zero or positive, then the rectifier output is essentially the input itself. If negative, then the output is zero. Mathematically, <br> \n",
    "    <center> $f(x) = max(0,x)$ </center>\n",
    "    <br>\n",
    "    ![title](ReLU.jpg)\n",
    "    <br> It is quite evident upon looking at the above plot that the slope of the graph for values lesser than or equal to 0, is 0, and for values greater than 0, 1. Therefore, the derivative of this function can be represented as, <br><br>\n",
    "    <center> $ f(x) = 0, if x <= 0 $ </center>\n",
    "    <center> $ f(x) = 1, if x > 0  $ </center>\n",
    "    \n",
    " - ### Implementation <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new class NeuralNetworkReLU which inherits from the NeuralNetwork class, and define new functions \n",
    "# activation and activationDerivate which implement the Rectified Linear function. \n",
    "class NeuralNetworkReLU(nn.NeuralNetwork):\n",
    "    def __init__(self, ni, nh, no):\n",
    "        super(NeuralNetworkReLU, self).__init__(ni, nh, no)\n",
    "\n",
    "    def activation(self, weighted_sum):\n",
    "        return np.maximum(0, weighted_sum)\n",
    "\n",
    "    def activationDerivative(self, activation_value):\n",
    "        actDer = np.copy(activation_value)\n",
    "        actDer[actDer <= 0] = 0\n",
    "        actDer[actDer > 0] = 1\n",
    "        return actDer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Comparision with tanH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - ### Plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the function *partition* as shown below. The primary purpose of this function is to take in input data, and the desired target output, and divide the records into training and testing data, based on the fraction argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(X, T, fraction, shuffle):\n",
    "    nRows = X.shape[0]\n",
    "    # Choose number of rows for training and testing data.\n",
    "    nTrain = int(round(fraction*nRows)) \n",
    "    nTest = nRows - nTrain\n",
    "\n",
    "    rows = np.arange(nRows)\n",
    "    # If the shuffle argument is set to true, then mix up the data records randomly.\n",
    "    if(shuffle == True):\n",
    "        np.random.shuffle(rows)\n",
    "\n",
    "    trainIndices = rows[:nTrain]\n",
    "    testIndices = rows[nTrain:]\n",
    "\n",
    "    Xtrain = X[trainIndices, :]\n",
    "    Ttrain = T[trainIndices, :]\n",
    "    Xtest = X[testIndices, :]\n",
    "    Ttest = T[testIndices, :]\n",
    "    \n",
    "    return Xtrain, Ttrain, Xtest, Ttest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is to determine the Root Mean Squared Error for any two input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(A, B):\n",
    "    return np.sqrt(np.mean((A - B)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out a few examples to demonstrate the working of the partition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(10*2).reshape((10, 2))\n",
    "T = X[:, 0:1] * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 2,  3],\n",
       "       [ 4,  5],\n",
       "       [ 6,  7],\n",
       "       [ 8,  9],\n",
       "       [10, 11],\n",
       "       [12, 13],\n",
       "       [14, 15],\n",
       "       [16, 17],\n",
       "       [18, 19]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. ],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [0.6],\n",
       "       [0.8],\n",
       "       [1. ],\n",
       "       [1.2],\n",
       "       [1.4],\n",
       "       [1.6],\n",
       "       [1.8]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ttrain, Xtest, Ttest = partition(X, T, 0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 2,  3],\n",
       "       [ 4,  5],\n",
       "       [ 6,  7],\n",
       "       [ 8,  9],\n",
       "       [10, 11],\n",
       "       [12, 13],\n",
       "       [14, 15]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. ],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [0.6],\n",
       "       [0.8],\n",
       "       [1. ],\n",
       "       [1.2],\n",
       "       [1.4]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ttrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 17],\n",
       "       [18, 19]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6],\n",
       "       [1.8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ttest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples for ```shuffle=True```. The data samples in this case are rearranged randomly before partitioning is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ttrain, Xtest, Ttest = partition(X, T, 0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [18, 19],\n",
       "       [12, 13],\n",
       "       [14, 15],\n",
       "       [ 4,  5],\n",
       "       [16, 17],\n",
       "       [ 6,  7],\n",
       "       [ 2,  3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. ],\n",
       "       [1.8],\n",
       "       [1.2],\n",
       "       [1.4],\n",
       "       [0.4],\n",
       "       [1.6],\n",
       "       [0.6],\n",
       "       [0.2]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ttrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  9],\n",
       "       [10, 11]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8],\n",
       "       [1. ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ttest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the RMSE values for the following cases:\n",
    "1. Using tanh activation function, and calculating RMSE on training data.\n",
    "2. Using tanh activation function, and calculating RMSE on testing data.\n",
    "3. Using ReLU activation function and calculating RMSE on training data.\n",
    "4. Using ReLU activation function and calculating RMSE on training data.\n",
    "\n",
    "In the following snippet of code, we take each of the two activation functions, train using them with different hidden layer structures, 10 times for each structure, and store the RMSE mean in each case. Finally, we plot everything, with four different curves for each of the above cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv data.\n",
    "dframe = pd.read_csv('energydata_complete.csv', sep=',',header=None)\n",
    "# Filter out required columns.\n",
    "#dframe = dframe.drop(dframe.columns[[0, -2, -1]], axis=1)\n",
    "\n",
    "# Get target.\n",
    "Td = dframe.iloc[1:, [1]]\n",
    "Td = Td.as_matrix()\n",
    "T = Td.astype(float)\n",
    "\n",
    "# Get input.\n",
    "Xd = dframe.iloc[1:, 2:-2]\n",
    "Xd = Xd.as_matrix()\n",
    "X = Xd.astype(float)\n",
    "\n",
    "# Comparision\n",
    "hiddenLayers = [[u]*nl for u in [1, 2, 5, 10, 50] for nl in [1, 2, 3, 4, 5, 10]]\n",
    "tanHlist = []\n",
    "ReLUlist = []\n",
    "for actFun in [nn.NeuralNetwork, NeuralNetworkReLU]:\n",
    "    for hidden in hiddenLayers:\n",
    "        # Create list for storing RMSE.\n",
    "        rmseTrainList = []\n",
    "        rmseTestList = [] \n",
    "        for i in range(10):\n",
    "            Xtrain, Ttrain, Xtest, Ttest = partition(X, T, 0.8, shuffle = False)\n",
    "            nnet = actFun(Xtrain.shape[1], hidden, Ttrain.shape[1])\n",
    "            nnet.train(Xtrain, Ttrain, 100)\n",
    "            rmseTrain = rmse(Ttrain, nnet.use(Xtrain))\n",
    "            rmseTest = rmse(Ttest, nnet.use(Xtest))\n",
    "            rmseTrainList.append(rmseTrain)\n",
    "            rmseTestList.append(rmseTest)\n",
    "        rmseTrainMean = sum(rmseTrainList)/len(rmseTrainList)\n",
    "        rmseTestMean = sum(rmseTestList)/len(rmseTestList)\n",
    "        if(actFun == nn.NeuralNetwork):\n",
    "            tanHlist.append([hidden, rmseTrainMean, rmseTestMean])\n",
    "        else:\n",
    "            ReLUlist.append([hidden, rmseTrainMean, rmseTestMean])\n",
    "\n",
    "tanHlist = pd.DataFrame(tanHlist)\n",
    "ReLUlist = pd.DataFrame(ReLUlist)\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "plt.plot(tanHlist.values[:, 1], 'b', label = 'tanH Train RMSE')\n",
    "plt.plot(tanHlist.values[:, 2], 'g', label = 'tanH Test RMSE')\n",
    "plt.plot(ReLUlist.values[:, 1], 'm', label = 'ReLU Train RMSE')\n",
    "plt.plot(ReLUlist.values[:, 2], 'k', label = 'ReLU Test RMSE')\n",
    "#plt.plot(tanHlist.values[:, 1:], 'o-')\n",
    "#plt.plot(ReLUlist.values[:, 1:], 'o-')\n",
    "plt.legend(('tanh Train RMSE', 'tanh Test RMSE', 'ReLU Train RMSE', 'ReLU Test RMSE'))\t\n",
    "plt.xticks(range(tanHlist.shape[0]), hiddenLayers, rotation=30, horizontalalignment='right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - ### Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - ### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. References\n",
    "[1] George E. Dahl, Tara N. Sainath and Geoffrey\n",
    "E. Hinton, â€œImproving Deep Neural Networks for LVCSR Using Rectified Linear Units\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading and Check-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your notebook will be run and graded automatically. Test this grading process by first downloading [A3grader.tar](http://www.cs.colostate.edu/~anderson/cs445/notebooks/A3grader.tar) and extract `A3grader.py` from it. Run the code in the following cell to demonstrate an example grading session. You should see a perfect execution score of  60 / 60 if your functions and class are defined correctly. The remaining 40 points will be based on the results you obtain from the comparisons of hidden layer structures and the two activation functions applied to the energy data.\n",
    "\n",
    "For the grading script to run correctly, you must first name this notebook as `Lastname-A3.ipynb` with `Lastname` being your last name, and then save this notebook.  Your working director must also contain `neuralnetworksA2.py` and `mlutilities.py` from lecture notes.\n",
    "\n",
    "Combine your notebook, `neuralnetworkA2.py`, and `mlutilities.py` into one zip file or tar file.  Name your tar file `Lastname-A3.tar` or your zip file `Lastname-A3.zip`.  Check in your tar or zip file using the `Assignment 3` link in Canvas.\n",
    "\n",
    "A different, but similar, grading script will be used to grade your checked-in notebook. It will include other tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================= Code Execution =======================\n",
      "\n",
      "Extracting python code from notebook named 'Madharapakkam Pagadala-A3.ipynb' and storing in notebookcode.py\n",
      "Removing all statements that are not function or class defs or import statements.\n",
      "\n",
      "Testing  import neuralnetworksA2 as nn\n",
      "\n",
      "--- 5/5 points. The statement  import neuralnetworksA2 as nn  works.\n",
      "\n",
      "Testing nnet = nn.NeuralNetwork(1, 10, 1)\n",
      "\n",
      "--- 5/5 points. nnet correctly constructed\n",
      "\n",
      "Testing a = nnet.activation(-0.8)\n",
      "\n",
      "--- 5/5 points. activation of -0.664036770267849 is correct.\n",
      "\n",
      "Testing da = nnet.activationDerivative(-0.664)\n",
      "\n",
      "--- 5/5 points. activationDerivative of 0.5591039999999999 is correct.\n",
      "\n",
      "Testing nnetrelu = NeuralNetworkReLU(1, 5, 1)\n",
      "\n",
      "--- 5/5 points. nnet correctly constructed\n",
      "\n",
      "Testing a = nnetrelu.activation(-0.8)\n",
      "\n",
      "--- 5/5 points. activation of 0.0 is correct.\n",
      "\n",
      "Testing a = nnetrelu.activation(1.8)\n",
      "\n",
      "--- 5/5 points. activation of 1.8 is correct.\n",
      "\n",
      "Testing da = nnetrelu.activationDerivative(0.0)\n",
      "\n",
      "--- 5/5 points. activationDerivative of 0.0 is correct.\n",
      "\n",
      "Testing da = nnetrelu.activationDerivative(5.5)\n",
      "\n",
      "--- 5/5 points. activationDerivative of 1.0 is correct.\n",
      "\n",
      "Testing X = np.arange(18).reshape((6, 3))\n",
      "        T = X[:,0:1] - X[:, 2:3]\n",
      "        Xtrain, Ttrain, Xtest, Ttest = partition(X, T, 0.5, shuffle=False)\n",
      "\n",
      "--- 15/15 points. partition result is correct.\n",
      "\n",
      "C:\\Users\\vigne\\Documents\\Colorado State University\\Courses\\CS445 - Machine Learning\\Assignment_Notebooks\\Assignment_3 Execution Grade is 60/60\n",
      "\n",
      "============= Experiments with Energy Data =============\n",
      "\n",
      "--- _/20 points. Correct implementation of procedure for collecting RMSEs averaged over 10 runs.\n",
      "Comments:\n",
      "\n",
      "--- _/10 points. Correct plot with four curves for training and testing RMSEs for the two activation functions.\n",
      "                 Label y axis as \"RMSE\".\n",
      "                 Label x axis with hidden layer structures rotated a bit so they can be read.\n",
      "                 Include legend with brief label for each curve.\n",
      "Comments:\n",
      "\n",
      "--- _/10 points. Discussion of what you observe in the plot.\n",
      "Comments:\n",
      "\n",
      "C:\\Users\\vigne\\Documents\\Colorado State University\\Courses\\CS445 - Machine Learning\\Assignment_Notebooks\\Assignment_3 Notebook Grade is __ / 40\n",
      "\n",
      "C:\\Users\\vigne\\Documents\\Colorado State University\\Courses\\CS445 - Machine Learning\\Assignment_Notebooks\\Assignment_3 FINAL GRADE is __ / 100\n"
     ]
    }
   ],
   "source": [
    "%run -i A3grader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run additional experiments using different numbers of training iterations.  How do the relative performances of the three activation functions depend on numbers of training iterations?  This will earn one extra credit point.\n",
    "\n",
    "You may also earn an extra credit point by creating yet another version of the neural network class, called ```NeuralNetworkSwish``` and repeat the above comparisons.  You may set the constant $\\beta = 1$.  This is tricker than it sounds, because the Swish activation derivative requires the weighted sum as an argument, but our other two activation function derivatives did not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
