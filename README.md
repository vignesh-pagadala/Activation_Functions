# Activation Functions

Comparing the ReLU and tanH activation functions to see which one leads to better performance.

---

## About

This project implements the following activation functions:

1. Rectified Linear Unit (ReLU)
2. Hyperbolic Tangent (tanH)

The objective to compare their performance using a sample dataset.

This project was implemented as a part of the CS445: Introduction to Machine Learning course at Colorado State University, during the Spring of 2018.  

### Built With

* Python
* Jupyter Notebook

## Getting Started

### Prerequisites

* Anaconda distribution: https://www.anaconda.com/products/individual

### Installation

To install, clone the GitHub repo to a local directory:

`git clone https://github.com/vignesh-pagadala/activation-functions.git`

## Usage

Run with Jupyer Notebook:

`jupyter-notebook Activation-Functions.ipynb`

## Roadmap

See the [open issues](https://github.com/vignesh-pagadala/activation-functions/issues) for a list of proposed features (and known issues).

- [Top Feature Requests](https://github.com/vignesh-pagadala/activation-functions/issues?q=label%3Aenhancement+is%3Aopen+sort%3Areactions-%2B1-desc) (Add your votes using the üëç reaction)
- [Top Bugs](https://github.com/vignesh-pagadala/activation-functions/issues?q=is%3Aissue+is%3Aopen+label%3Abug+sort%3Areactions-%2B1-desc) (Add your votes using the üëç reaction)
- [Newest Bugs](https://github.com/vignesh-pagadala/activation-functions/issues?q=is%3Aopen+is%3Aissue+label%3Abug)

## Support

Reach out to the maintainer at one of the following places:

- [GitHub issues](https://github.com/vignesh-pagadala/activation-functions/issues/new?assignees=&labels=question&template=04_SUPPORT_QUESTION.md&title=support%3A+)
- The email which is located [in GitHub profile](https://github.com/vignesh-pagadala)

## Project assistance

If you want to say **thank you** or/and support active development of Activation Functions:

- Add a [GitHub Star](https://github.com/vignesh-pagadala/activation-functions) to the project.
- Tweet about the Activation Functions on your Twitter.
- Write interesting articles about the project on [Dev.to](https://dev.to/), [Medium](https://medium.com/) or personal blog.

Together, we can make Activation Functions **better**!

## Contributing

First off, thanks for taking the time to contribute! Contributions are what make the open-source community such an amazing place to learn, inspire, and create. Any contributions you make will benefit everybody else and are **greatly appreciated**.

We have set up a separate document containing our [contribution guidelines](docs/CONTRIBUTING.md).

Thank you for being involved!

## Authors & contributors

The original setup of this repository is by [Vignesh Pagadala](https://github.com/vignesh-pagadala).

For a full list of all authors and contributors, check [the contributor's page](https://github.com/vignesh-pagadala/activation-functions/contributors).

## Security

Activation Functions follows good practices of security, but 100% security can't be granted in software.
Activation Functions is provided **"as is"** without any **warranty**. Use at your own risk.

_For more info, please refer to the [security](docs/SECURITY.md)._

## License

This project is licensed under the **GNU General Public License v3**.

See [LICENSE](LICENSE) for more information.
